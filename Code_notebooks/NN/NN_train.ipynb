{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"NN_train.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"rLP6uHZvkUbG"},"source":["import numpy as np\n","import librosa \n","import glob\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import tensorflow as tf\n","from scipy.stats import multivariate_normal"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"12Za1bO3kUbK"},"source":["def readAudio(filename,Fs):\n","    x, sr = librosa.load(filename, sr=Fs)\n","    return x, sr\n","\n","#calculate spectrogram\n","def calc_spec(x):\n","    n_fft = 1024\n","    hop_length = 512\n","    win_length = 1024\n","    X = np.abs(librosa.stft(x, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window='hann', dtype = np.complex256))\n","    X = librosa.power_to_db(X**2,ref=np.max)\n","    return X\n","\n","def saveSpectrogram(X, outfilename):\n","    assert outfilename[-4:]=='.npy'  #'outfilename extension should be .npy'\n","    np.save(outfilename, X)\n","    return\n","\n","def readSpectrogram(infilename):\n","    X = np.load(infilename)\n","    return X\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERWjhmLukUbL"},"source":["def read_and_combine_speech_music_spec():\n","  p=0\n","  for i in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '14', '15', '16', '17', '18']:\n","    name = 'music_samples'+'/' + i + '.wav'\n","    print(name)\n","    audio, sr = readAudio(name,16000)\n","    if p==0:\n","      music_data = audio\n","      p=1\n","    elif p==1:\n","      music_data = np.hstack((music_data,audio))\n","  p=0\n","  for i in ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30']:\n","    name = 'speech_samples'+'/' + i + '.wav'\n","    print(name)\n","    audio, sr = readAudio(name,16000)\n","    if p==0:\n","      speech_data = audio\n","      p=1\n","    elif p==1:\n","      speech_data = np.hstack((speech_data,audio))\n","  speech_spec=calc_spec(speech_data)\n","  label_speech = np.zeros((speech_spec.shape[1],))\n","  music_spec=calc_spec(music_data)\n","  label_music = np.ones((music_spec.shape[1],))\n","  label = np.hstack([label_speech,label_music])\n","  combine = np.hstack([speech_spec,music_spec])\n","  return speech_spec, music_spec, label, combine"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"05MmQtYTkUbM"},"source":["def define_NN():\n","  model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n","                                    tf.keras.layers.Dense(513, activation=tf.nn.relu), \n","                                    tf.keras.layers.Dense(64, activation=tf.nn.relu), \n","                                    tf.keras.layers.Dense(32, activation=tf.nn.relu), \n","                                    tf.keras.layers.Dense(2, activation=tf.nn.softmax)])\n","  model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsWHPJxOkUbN","outputId":"cff6ddbe-0d66-4304-b56f-6431dd1a9361"},"source":["if __name__==\"__main__\":\n","  speech_spec, music_spec, label, combine = read_and_combine_speech_music_spec()\n","  model = define_NN()\n","  model.fit(combine.T, label, epochs=20)\n","  model.save('NN_model')\n","  #l=label_timestamp_NN(model,spec,time_stamp)\n","  #for i  in range(0,time_stamp.shape[0]):\n","  #  time_stamp[i,1]= ((time_stamp[i,1]-1)*512 + 1024)/16000\n","  #  time_stamp[i,2]= ((time_stamp[i,2]-1)*512 + 1024)/16000\n","  #final_time_stamp_NN=np.hstack([time_stamp,l])\n","  #df = pd.read_csv('labels.csv')\n","  #print(final_time_stamp_NN,'orig', df)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["music_samples/1.wav\n","music_samples/2.wav\n","music_samples/3.wav\n","music_samples/4.wav\n","music_samples/5.wav\n","music_samples/6.wav\n","music_samples/7.wav\n","music_samples/8.wav\n","music_samples/9.wav\n","music_samples/10.wav\n","music_samples/11.wav\n","music_samples/12.wav\n","music_samples/14.wav\n","music_samples/15.wav\n","music_samples/16.wav\n","music_samples/17.wav\n","music_samples/18.wav\n","speech_samples/1.wav\n","speech_samples/2.wav\n","speech_samples/3.wav\n","speech_samples/4.wav\n","speech_samples/5.wav\n","speech_samples/6.wav\n","speech_samples/7.wav\n","speech_samples/8.wav\n","speech_samples/9.wav\n","speech_samples/10.wav\n","speech_samples/11.wav\n","speech_samples/12.wav\n","speech_samples/14.wav\n","speech_samples/15.wav\n","speech_samples/16.wav\n","speech_samples/17.wav\n","speech_samples/18.wav\n","speech_samples/19.wav\n","speech_samples/20.wav\n","speech_samples/21.wav\n","speech_samples/22.wav\n","speech_samples/23.wav\n","speech_samples/24.wav\n","speech_samples/25.wav\n","speech_samples/26.wav\n","speech_samples/27.wav\n","speech_samples/28.wav\n","speech_samples/29.wav\n","speech_samples/30.wav\n","Epoch 1/20\n","2155/2155 [==============================] - 6s 2ms/step - loss: 0.9089 - accuracy: 0.8156\n","Epoch 2/20\n","2155/2155 [==============================] - 7s 3ms/step - loss: 0.3575 - accuracy: 0.8691\n","Epoch 3/20\n","2155/2155 [==============================] - 5s 3ms/step - loss: 0.2541 - accuracy: 0.8937\n","Epoch 4/20\n","2155/2155 [==============================] - 5s 3ms/step - loss: 0.2428 - accuracy: 0.9008\n","Epoch 5/20\n","2155/2155 [==============================] - 5s 2ms/step - loss: 0.2062 - accuracy: 0.9168\n","Epoch 6/20\n","2155/2155 [==============================] - 5s 2ms/step - loss: 0.1941 - accuracy: 0.9205\n","Epoch 7/20\n","2155/2155 [==============================] - 5s 3ms/step - loss: 0.1864 - accuracy: 0.9246\n","Epoch 8/20\n","2155/2155 [==============================] - 5s 3ms/step - loss: 0.1658 - accuracy: 0.9330\n","Epoch 9/20\n","2155/2155 [==============================] - 5s 3ms/step - loss: 0.1535 - accuracy: 0.9385\n","Epoch 10/20\n","2155/2155 [==============================] - 5s 3ms/step - loss: 0.1452 - accuracy: 0.9417\n","Epoch 11/20\n","2155/2155 [==============================] - 5s 2ms/step - loss: 0.1390 - accuracy: 0.9452\n","Epoch 12/20\n","2155/2155 [==============================] - 6s 3ms/step - loss: 0.1374 - accuracy: 0.9444\n","Epoch 13/20\n","2155/2155 [==============================] - 5s 3ms/step - loss: 0.1308 - accuracy: 0.9486\n","Epoch 14/20\n","2155/2155 [==============================] - 5s 2ms/step - loss: 0.1288 - accuracy: 0.9500\n","Epoch 15/20\n","2155/2155 [==============================] - 5s 3ms/step - loss: 0.1237 - accuracy: 0.9510\n","Epoch 16/20\n","2155/2155 [==============================] - 6s 3ms/step - loss: 0.1217 - accuracy: 0.9519\n","Epoch 17/20\n","2155/2155 [==============================] - 5s 3ms/step - loss: 0.1182 - accuracy: 0.9537\n","Epoch 18/20\n","2155/2155 [==============================] - 5s 3ms/step - loss: 0.1140 - accuracy: 0.9554\n","Epoch 19/20\n","2155/2155 [==============================] - 5s 2ms/step - loss: 0.1135 - accuracy: 0.9556\n","Epoch 20/20\n","2155/2155 [==============================] - 6s 3ms/step - loss: 0.1135 - accuracy: 0.9551\n","INFO:tensorflow:Assets written to: NN_model/assets\n"]}]},{"cell_type":"code","metadata":{"id":"uS0lTIbbkUbP"},"source":[""],"execution_count":null,"outputs":[]}]}